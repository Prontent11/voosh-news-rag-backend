# ğŸ“° News RAG Chatbot â€” Backend

A production-ready backend for a **Retrieval-Augmented Generation (RAG)** chatbot that answers questions using **recent news articles**.

The system ingests news data, generates embeddings using **Gemini**, stores vectors in **Qdrant**, and serves grounded responses through a **session-based chat API** backed by **Upstash Redis**.

---

## ğŸš€ Key Features

### ğŸ” Retrieval-Augmented Generation (RAG)
- Offline ingestion of news articles
- Text chunking and vector embeddings
- Semantic search using Qdrant
- Context-grounded answer generation

### ğŸ’¬ Chat API
- Session-based conversations
- Redis-backed chat history
- Resettable chat sessions

### ğŸ§  Hallucination-Safe by Design
- The LLM answers **only from retrieved context**
- If information is not present, the system responds with *"I don't know"*
- Small talk and identity questions are routed outside the RAG pipeline

### â˜ï¸ Cloud Ready
- Qdrant Cloud for vector storage
- Upstash Redis for session management
- Fully environment-driven configuration
- Deployed on Render

---

## ğŸ—ï¸ High-Level Architecture

```
User Query
    â”‚
    â–¼
[ Express API ]
    â”‚
    â”‚
    â””â”€ Knowledge Query
         â”‚
         â–¼
   [ Gemini Embeddings ]
         â”‚
         â–¼
   [ Qdrant Vector Search ]
         â”‚
         â–¼
   [ Relevant Context ]
         â”‚
         â–¼
   [ Gemini 1.5 Flash ]
         â”‚
         â–¼
   Grounded Answer
```

---

## ğŸ› ï¸ Tech Stack

| Layer              | Technology                    |
|--------------------|-------------------------------|
| Language           | TypeScript                    |
| Runtime            | Node.js                       |
| Web Framework      | Express                       |
| Vector Database    | Qdrant Cloud                  |
| Cache / Sessions   | Redis (Upstash)               |
| Embeddings         | Gemini `text-embedding-004`   |
| LLM                | Gemini `2.5-flash`            |
| Deployment         | Render                        |

---

## ğŸ“ Project Structure

```
src/
â”œâ”€ chat/
â”‚  â””â”€ chatService.ts          # Chat orchestration & session handling
â”‚
â”œâ”€ embeddings/
â”‚  â””â”€ geminiEmbedding.ts      # Gemini embedding logic
â”‚
â”œâ”€ generation/
â”‚  â””â”€ answerGeneration.ts         # Answer generation using LLM
â”‚
â”œâ”€ retrieval/
â”‚  â””â”€ retrieveContext.ts      # Vector search logic (Qdrant)
â”‚
â”œâ”€ scripts/
â”‚  â””â”€ inngestNews.ts       # Offline news ingestion pipeline
â”‚
â”œâ”€ config/
â”‚  â”œâ”€ redis.ts                # Upstash Redis configuration
â”‚  â””â”€ qdrant.ts               # Qdrant Cloud configuration
â”‚  â””â”€ genAI.ts                # Gemini AI instance
â”œâ”€ routes/
â”‚  â””â”€ chat.routes.ts          # Express API routes
â”‚
â”œâ”€ utils/
â”‚  â””â”€ chunkText.ts            # Text chunking utility
â”‚
â””â”€ server.ts                  # Application entry point
```

---

## âš™ï¸ Environment Variables

Create a `.env` file in the project root:

```env
# Server
SERVER_PORT=5000

# Gemini
GEMINI_API_KEY=your_gemini_api_key

# Qdrant Cloud
QDRANT_URL=https://your-cluster.region.cloud.qdrant.io
QDRANT_API_KEY=your_qdrant_api_key

# Upstash Redis
REDIS_URL=rediss://default:password@host:6379
```

> âš ï¸ **Do not commit `.env` files.**  
> All secrets are injected via environment variables in production.

---

## ğŸš€ Running Locally

### 1. Install Dependencies

```bash
npm install
```

### 2. Create Qdrant Collection

Before running the ingestion pipeline, create a collection in Qdrant Cloud.

**Option A: Using the Script (Recommended)**
```bash
npx ts-node-dev src/config/qdrant.ts
```

**Option B: Manual Setup**
1. Log in to your [Qdrant Cloud Dashboard](https://cloud.qdrant.io)
2. Create a new collection named **`news_articles`**
3. Set vector size to **768** (Gemini text-embedding-004 dimension)
4. Choose distance metric: **Cosine**

### 3. Run the Ingestion Pipeline

Now populate Qdrant with news data:

```bash
npx ts-node-dev scripts/ingestNews.ts
```

**Pipeline steps:**
1. Fetch news articles (RSS / HTML)
2. Clean and chunk text
3. Generate embeddings using Gemini
4. Store vectors in Qdrant Cloud collection

### 3. Start Development Server

```bash
npm run dev
```

Server will run at:
```
http://localhost:5000
```

### 4. Health Check

```bash
GET /health
```

---

## ğŸ“¡ API Endpoints

### ğŸ”¹ POST `/api/chat`

Send a message to the chatbot.

**Request:**
```json
{
  "message": "What is happening in global markets?",
  "sessionId": "optional-session-id"
}
```

**Response:**
```json
{
  "sessionId": "uuid",
  "answer": "Based on recent news articles..."
}
```

---

### ğŸ”¹ GET `/api/history/:sessionId`

Fetch chat history for a session.

**Response:**
```json
[
  { "role": "user", "content": "What is happening in global markets?" },
  { "role": "assistant", "content": "Based on recent news articles..." }
]
```

---

### ğŸ”¹ DELETE `/api/reset/:sessionId`

Reset a chat session.

**Response:**
```json
{
  "status": "reset",
  "sessionId": "uuid"
}
```

---

## ğŸ§  Design Decisions

### Why RAG?
- Prevents hallucinations
- Ensures answers are grounded in real data
- Makes responses explainable and auditable

### Why Redis for Sessions?
- Stateless API design
- Easy horizontal scaling
- Built-in session TTL support

### Why Intent-Based Routing?
- Improves UX for greetings and identity questions
- Keeps RAG pipeline strictly factual
- Faster responses for non-knowledge queries

---

## â˜ï¸ Deployment (Render)

The backend is deployed on Render.

### Production Setup
- **Build Command:** `npm install && npm run build`
- **Start Command:** `npm start`
- **Port:** `process.env.PORT`
- **Secrets:** Injected via Render environment variables

### Health Check
```bash
GET /health
```

---

## ğŸ”® Future Improvements

- [ ] Token streaming with WebSockets
- [ ] Source attribution in responses
- [ ] Smarter intent classification
- [ ] Scheduled ingestion jobs
- [ ] Authentication & user profiles
- [ ] Multi-language support
- [ ] Advanced analytics dashboard

---

## âœ… Current Status

**v1.0 â€” Feature Complete**

Includes:
- âœ… Full RAG pipeline
- âœ… Session-based chat API
- âœ… Redis-backed memory
- âœ… Cloud deployment readiness
- âœ… Production error handling

---

## ğŸ“Œ Project Status

- âœ”ï¸ Backend complete
- âœ”ï¸ Deployed on Render
- âœ”ï¸ Ready for frontend integration

---

## ğŸ“„ License

MIT License - feel free to use this project for your own applications.

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

---

## ğŸ“§ Support

For questions or suggestions, please open an issue on GitHub.

---

**Built by Yash Yadao**
